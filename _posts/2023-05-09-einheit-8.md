---
title: "Metadaten modellieren und Schnittstellen nutzen"
date: 2023-05-09
---

In der heutigen Lerneinheit werden wir die Modellierung von Metadaten und die Nutzung von Schnittstellen anschauen. Wir werden heute auch auf das Programm MarcEdit eingehen und weitere Tools f√ºr die Metadatentransformation kennenlernen. Am Anfang gab es einen zus√§tzlichen Input zur OAI-PMH Schnittstelle, Diamond Open Access, ORCID, ResearchGate und Zweitver√∂ffentlichungsrecht. Diese Zusatzinformationen fand ich noch sehr spannend.

Die drei weitverbreitetsten Austauschprotokolle f√ºr Metadaten sind Z39.50, SRU und OAI-PMH. Als kleine √úbung werden wir mithilfe vom VuFindHarvest Metadaten von Koha, ArchivesSpace und DSpace einsammeln. Dies war f√ºr mich eine gute Wiederholung der bereits angeschauten Lerneinheiten. Ein Kommilitone hat eine spannende These eingeworfen, dass Crosswalks vermutlich eher noch mehr Chaos verursachen, als es zu vermindern. Es kann selbstverst√§ndlich abschreckend erscheinen, wenn wir uns dem Problem bewusst sind, dass Metadaten bereits in ihren vorgesehen Programmen Fehler erhalten k√∂nnen. Bei einer √úbersetzung dieser Metadaten k√∂nnen daher mehr Fehler auftauchen.

XSLT wurde uns als Programmiersprache f√ºr XML-Dateien vorgestellt. Dieser Abschnitt war mir ein wenig zu technisch, wenn ich es so formulieren darf. Es war aber wirklich sehr spannend, diese Transformationen zu verfolgen und versuchen zu verstehen, wie sie funktionieren. Beim Versuch der Transformation von ArchivesSpace zu MARC21 kam es zu einer Fehlermeldung.

MarcEdit ist eine Software, welche die Metadaten-Transformation durchf√ºhren kann. Dieses Programm fand ich noch spannend, dass man EAD-Dateien nicht ohne Umweg zu MARC21 transformieren kann. Aber beim Versuch mit MarcEdit hat die Transformation besser funktioniert, aber auch dort kam es zu Informationsverlust.

Als √úbung haben wir danach CSV-Tabellendaten nach MARCXML mit OpenRefine transformiert. Wir mussten ein wenig warten, da die Codespaces von Github einen technischen Fehler hatten. Spannend war aber zu beobachten, wie ein Server f√ºr OpenRefine installiert wird. Anschliessend machten wir ein Reverse Engineering, dort sollten wir die Ausgangsdaten mit den Ergebnissen vergleichen und die Transformationen der jeweiligen Felder in eigenen Worten fassen. Fand ich pers√∂nlich eine spannende √úbung, um solche Transformationen zu vergleichen.

Anschliessend haben wir noch die XML-Dateien validiert. Da wir in den vorherigen √úbungen gemerkt haben, dass Fehler bei Transformationen auftauchen, ist so eine Validierung wichtig. Online findet man solche Validatoren, welche die Daten √ºberpr√ºfen k√∂nnen.

Wir lernten noch den ETL-Prozess kennen. Mit diesem Prozess werden Daten von einem Repository extrahiert, transformiert und in einem System geladen. Spannend auch, dass wir noch alternative Software angetroffen haben, wie Catmandu (nicht in Nepal üòä) und Metafacture. Metafacture verwendet als Programmiersprache Java. Und auch mit Python k√∂nnte man solche Transformationssoftware programmieren. F√ºr mich pers√∂nlich ist dies zu technisch, aber ich fand die Einsicht in diesen vielf√§ltigen Programmen sehr spannend.

Abschliessend haben wir noch die JSON-APIs besprochen. Moderne APIs liefern die Daten in JSON anstelle von XML aus.

Zum Abschluss noch ein mit Gencraft generiertes Bild zu Kathmandu, aber nur mit Katzen:

